{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alberto-paparella/nrr95p/blob/main/vdm_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3WVe91iTMPw"
      },
      "source": [
        "# Video Diffusion Model Training\n",
        "\n",
        "This notebook is meant to experiment with the extracted dataset aiming to train a video diffusion model able to predict, given the first few frames of a gif showing the nrr95p evolution across some days, the next frames in the gif (i.e., the possible evolution of data in the next few days)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDq-K4ZeTMPy"
      },
      "source": [
        "In the first experiment, a video diffusion model is trained with 2018 data; then, the model is evaluated on 2019 data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS16XEbUTMP0"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU25YfxMTMP1"
      },
      "source": [
        "Let's start by installing the [video_diffusion_pytorch](https://github.com/lucidrains/video-diffusion-pytorch) implementation of video diffusion models for pytorch developed by [Phil Wang](https://github.com/lucidrains)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yQKRQstTMP1"
      },
      "outputs": [],
      "source": [
        "%pip install video-diffusion-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup on Google Colab"
      ],
      "metadata": {
        "id": "ka9e-HMRTQO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To connect Google Drive (GDrive) with Colab, execute the following two lines of code in Colab:"
      ],
      "metadata": {
        "id": "oPAoIHIZTbB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "id": "TstcudRkTT3p",
        "outputId": "c5ea8052-0ad2-485f-d49e-849ec4246b8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the above shell will return a URL link and ask for an authorization code. Follow to the mentioned link, sign in Google account, and copy the authorization code by clicking at highlighted spot. Paste the authorization code in the shell and finally, Google Drive will be mounted at /content/gdrive. Note that, files in the drive are under the folder /content/gdrive/My Drive/. Now, we can import files in GDrive using a library like Pandas."
      ],
      "metadata": {
        "id": "LJarlMo0ThYS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgYjfByHTMP2"
      },
      "source": [
        "## Import 2018 gifs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pLYBk20TMP3"
      },
      "source": [
        "However, the installed implementation requires a different structure; refer to below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhMakrq5TMP3"
      },
      "outputs": [],
      "source": [
        "import imageio.v3 as iio\n",
        "from pathlib import Path\n",
        "\n",
        "videos = []#list()\n",
        "for video in Path(\"nrr95p/2018_gifs_a\").iterdir():\n",
        "    if not video.is_file():\n",
        "        continue\n",
        "    # index=None means: read all images in the file and stack along first axis\n",
        "    videos.append(iio.imread(video, index=None))\n",
        "for video in Path(\"nrr95p/2018_gifs_b\").iterdir():\n",
        "    if not video.is_file():\n",
        "        continue\n",
        "    # index=None means: read all images in the file and stack along first axis\n",
        "    videos.append(iio.imread(video, index=None))\n",
        "for video in Path(\"nrr95p/2018_gifs_c\").iterdir():\n",
        "    if not video.is_file():\n",
        "        continue\n",
        "    # index=None means: read all images in the file and stack along first axis\n",
        "    videos.append(iio.imread(video, index=None))\n",
        "\n",
        "# ndarray with (num_frames, height, width, channel)\n",
        "print(videos[0].shape)  # (36, 150, 200, 3)\n",
        "print(len(videos))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiYk0Ok9TMP5"
      },
      "source": [
        "## From gifs to tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prbtf2GzTMP6"
      },
      "source": [
        "Define function to convert gif to tensor; the function takes the gif path as an argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jsGcLSeRTMP6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image, ImageSequence\n",
        "\n",
        "CHANNELS_TO_MODE = {\n",
        "    1 : 'L',\n",
        "    3 : 'RGB',\n",
        "    4 : 'RGBA'\n",
        "}\n",
        "\n",
        "def seek_all_images(img, channels = 3):\n",
        "    assert channels in CHANNELS_TO_MODE, f'channels {channels} invalid'\n",
        "    mode = CHANNELS_TO_MODE[channels]\n",
        "\n",
        "    i = 0\n",
        "    while True:\n",
        "        try:\n",
        "            img.seek(i)\n",
        "            yield img.convert(mode)\n",
        "        except EOFError:\n",
        "            break\n",
        "        i += 1\n",
        "\n",
        "def gif_to_tensor(path, channels = 3, transform = T.ToTensor()):\n",
        "    img = Image.open(path)\n",
        "    tensors = tuple(map(transform, seek_all_images(img, channels = channels)))\n",
        "    return torch.stack(tensors, dim = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zc3aMUoaTMP7",
        "outputId": "8cab2dab-dbfc-4be7-8b2d-3c001064cedb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([125, 3, 10, 480, 640])\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "gifs = []\n",
        "for gif in Path(\"/content/gdrive/MyDrive/nrr95p/2018_gifs_a\").iterdir():\n",
        "    if not gif.is_file():\n",
        "        continue\n",
        "    gifs.append(gif_to_tensor(gif))\n",
        "\n",
        "videos = torch.stack(gifs)\n",
        "print(videos.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDM1LbIqTMP7"
      },
      "outputs": [],
      "source": [
        "# resize gif\n",
        "# Get sequence iterator\n",
        "frames = ImageSequence.Iterator(img)\n",
        "\n",
        "# Wrap on-the-fly thumbnail generator\n",
        "def thumbnails(frames):\n",
        "    for frame in frames:\n",
        "        thumbnail = frame.copy()\n",
        "        thumbnail.thumbnail([640,640], Image.ANTIALIAS)\n",
        "        yield thumbnail\n",
        "\n",
        "frames = thumbnails(frames)\n",
        "\n",
        "om = next(frames) # Handle first frame separately\n",
        "om.info = img.info # Copy sequence info\n",
        "om.save(\"out.gif\", save_all=True, append_images=list(frames))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVHoi8AATMP8"
      },
      "source": [
        "## Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOaRtAZvTMP8"
      },
      "outputs": [],
      "source": [
        "#import torch\n",
        "from video_diffusion_pytorch import Unet3D, GaussianDiffusion\n",
        "\n",
        "model = Unet3D(\n",
        "    dim = 64,\n",
        "    dim_mults = (1, 2, 4, 8)\n",
        ")\n",
        "\n",
        "diffusion = GaussianDiffusion(\n",
        "    model,\n",
        "    image_size = 640,\n",
        "    num_frames = 10,\n",
        "    timesteps = 1000,   # number of steps\n",
        "    loss_type = 'l1'    # L1 or L2\n",
        ")\n",
        "\n",
        "#videos = torch.randn(1, 3, 5, 32, 32) # video (batch, channels, frames, height, width) - normalized from -1 to +1\n",
        "loss = diffusion(videos)\n",
        "loss.backward()\n",
        "# after a lot of training\n",
        "\n",
        "sampled_videos = diffusion.sample(batch_size = 4)\n",
        "sampled_videos.shape # (4, 3, 5, 32, 32)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "1aaf1dd3f9839164dcd234dfa3d5790dd913a3d4e59c4052fb154b7e4ab27a60"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}