{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Diffusion Model Training\n",
    "\n",
    "This notebook is meant to experiment with the extracted dataset aiming to train a video diffusion model able to predict, given the first few frames of a gif showing the nrr95p evolution across some days, the next frames in the gif (i.e., the possible evolution of data in the next few days)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first experiment, a video diffusion model is trained with 2018 data; then, the model is evaluated on 2019 data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by installing the [video_diffusion_pytorch](https://github.com/lucidrains/video-diffusion-pytorch) implementation of video diffusion models for pytorch developed by [Phil Wang](https://github.com/lucidrains)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install video-diffusion-pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import 2018 gifs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the installed implementation requires a different structure; refer to below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v3 as iio\n",
    "from pathlib import Path\n",
    "\n",
    "videos = []#list()\n",
    "for video in Path(\"nrr95p/2018_gifs_a\").iterdir():\n",
    "    if not video.is_file():\n",
    "        continue\n",
    "    # index=None means: read all images in the file and stack along first axis\n",
    "    videos.append(iio.imread(video, index=None))\n",
    "for video in Path(\"nrr95p/2018_gifs_b\").iterdir():\n",
    "    if not video.is_file():\n",
    "        continue\n",
    "    # index=None means: read all images in the file and stack along first axis\n",
    "    videos.append(iio.imread(video, index=None))\n",
    "for video in Path(\"nrr95p/2018_gifs_c\").iterdir():\n",
    "    if not video.is_file():\n",
    "        continue\n",
    "    # index=None means: read all images in the file and stack along first axis\n",
    "    videos.append(iio.imread(video, index=None))\n",
    "\n",
    "# ndarray with (num_frames, height, width, channel)\n",
    "print(videos[0].shape)  # (36, 150, 200, 3)\n",
    "print(len(videos))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From gifs to tensors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to convert gif to tensor; the function takes the gif path as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image, ImageSequence\n",
    "\n",
    "CHANNELS_TO_MODE = {\n",
    "    1 : 'L',\n",
    "    3 : 'RGB',\n",
    "    4 : 'RGBA'\n",
    "}\n",
    "\n",
    "def seek_all_images(img, channels = 3):\n",
    "    assert channels in CHANNELS_TO_MODE, f'channels {channels} invalid'\n",
    "    mode = CHANNELS_TO_MODE[channels]\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        try:\n",
    "            img.seek(i)\n",
    "            yield img.convert(mode)\n",
    "        except EOFError:\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "def gif_to_tensor(path, channels = 3, transform = T.ToTensor()):\n",
    "    img = Image.open(path)\n",
    "    tensors = tuple(map(transform, seek_all_images(img, channels = channels)))\n",
    "    return torch.stack(tensors, dim = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([125, 3, 10, 480, 640])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "gifs = []\n",
    "for gif in Path(\"nrr95p/2018_gifs_a\").iterdir():\n",
    "    if not gif.is_file():\n",
    "        continue\n",
    "    gifs.append(gif_to_tensor(\"nrr95p/2018_gifs_a/nrr95p_2018_0_to_9.gif\"))\n",
    "\n",
    "videos = torch.stack(gifs)\n",
    "print(videos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize gif\n",
    "# Get sequence iterator\n",
    "frames = ImageSequence.Iterator(img)\n",
    "\n",
    "# Wrap on-the-fly thumbnail generator\n",
    "def thumbnails(frames):\n",
    "    for frame in frames:\n",
    "        thumbnail = frame.copy()\n",
    "        thumbnail.thumbnail([640,640], Image.ANTIALIAS)\n",
    "        yield thumbnail\n",
    "\n",
    "frames = thumbnails(frames)\n",
    "\n",
    "om = next(frames) # Handle first frame separately\n",
    "om.info = img.info # Copy sequence info\n",
    "om.save(\"out.gif\", save_all=True, append_images=list(frames))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "from video_diffusion_pytorch import Unet3D, GaussianDiffusion\n",
    "\n",
    "model = Unet3D(\n",
    "    dim = 64,\n",
    "    dim_mults = (1, 2, 4, 8)\n",
    ")\n",
    "\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size = 640,\n",
    "    num_frames = 10,\n",
    "    timesteps = 1000,   # number of steps\n",
    "    loss_type = 'l1'    # L1 or L2\n",
    ")\n",
    "\n",
    "#videos = torch.randn(1, 3, 5, 32, 32) # video (batch, channels, frames, height, width) - normalized from -1 to +1\n",
    "loss = diffusion(videos)\n",
    "loss.backward()\n",
    "# after a lot of training\n",
    "\n",
    "sampled_videos = diffusion.sample(batch_size = 4)\n",
    "sampled_videos.shape # (4, 3, 5, 32, 32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1aaf1dd3f9839164dcd234dfa3d5790dd913a3d4e59c4052fb154b7e4ab27a60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
