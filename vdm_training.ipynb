{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alberto-paparella/nrr95p/blob/main/vdm_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3WVe91iTMPw"
      },
      "source": [
        "# Video Diffusion Model Training\n",
        "\n",
        "This notebook is meant to experiment with the extracted dataset aiming to train a video diffusion model able to predict, given the first few frames of a gif showing the nrr95p evolution across some days, the next frames in the gif (i.e., the possible evolution of data in the next few days)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDq-K4ZeTMPy"
      },
      "source": [
        "In the first experiment, a video diffusion model is trained with 2018 data; then, the model is evaluated on 2019 data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS16XEbUTMP0"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU25YfxMTMP1"
      },
      "source": [
        "Let's start by installing the [video_diffusion_pytorch](https://github.com/lucidrains/video-diffusion-pytorch) implementation of video diffusion models for pytorch developed by [Phil Wang](https://github.com/lucidrains)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3yQKRQstTMP1",
        "outputId": "2cdf84d9-1c52-409f-acd6-e8687fd59be8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: video-diffusion-pytorch in /usr/local/lib/python3.9/dist-packages (0.6.0)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.9/dist-packages (from video-diffusion-pytorch) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from video-diffusion-pytorch) (0.14.1+cu116)\n",
            "Requirement already satisfied: einops>=0.4 in /usr/local/lib/python3.9/dist-packages (from video-diffusion-pytorch) (0.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from video-diffusion-pytorch) (4.65.0)\n",
            "Requirement already satisfied: rotary-embedding-torch in /usr/local/lib/python3.9/dist-packages (from video-diffusion-pytorch) (0.2.1)\n",
            "Requirement already satisfied: einops-exts in /usr/local/lib/python3.9/dist-packages (from video-diffusion-pytorch) (0.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->video-diffusion-pytorch) (4.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->video-diffusion-pytorch) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->video-diffusion-pytorch) (1.24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->video-diffusion-pytorch) (2.25.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->video-diffusion-pytorch) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->video-diffusion-pytorch) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->video-diffusion-pytorch) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->video-diffusion-pytorch) (4.0.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install video-diffusion-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup on Google Colab"
      ],
      "metadata": {
        "id": "ka9e-HMRTQO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To connect Google Drive (GDrive) with Colab, execute the following two lines of code in Colab:"
      ],
      "metadata": {
        "id": "oPAoIHIZTbB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "id": "TstcudRkTT3p",
        "outputId": "4609ade5-8540-4db9-f7ee-ff7dcf46d0eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the above shell will return a URL link and ask for an authorization code. Follow to the mentioned link, sign in Google account, and copy the authorization code by clicking at highlighted spot. Paste the authorization code in the shell and finally, Google Drive will be mounted at /content/gdrive. Note that, files in the drive are under the folder /content/gdrive/My Drive/. Now, we can import files in GDrive using a library like Pandas."
      ],
      "metadata": {
        "id": "LJarlMo0ThYS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgYjfByHTMP2"
      },
      "source": [
        "## Import 2018 gifs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pLYBk20TMP3"
      },
      "source": [
        "However, the installed implementation requires a different structure; refer to below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhMakrq5TMP3"
      },
      "outputs": [],
      "source": [
        "import imageio.v3 as iio\n",
        "from pathlib import Path\n",
        "\n",
        "videos = []#list()\n",
        "for video in Path(\"nrr95p/2018_gifs_a\").iterdir():\n",
        "    if not video.is_file():\n",
        "        continue\n",
        "    # index=None means: read all images in the file and stack along first axis\n",
        "    videos.append(iio.imread(video, index=None))\n",
        "for video in Path(\"nrr95p/2018_gifs_b\").iterdir():\n",
        "    if not video.is_file():\n",
        "        continue\n",
        "    # index=None means: read all images in the file and stack along first axis\n",
        "    videos.append(iio.imread(video, index=None))\n",
        "for video in Path(\"nrr95p/2018_gifs_c\").iterdir():\n",
        "    if not video.is_file():\n",
        "        continue\n",
        "    # index=None means: read all images in the file and stack along first axis\n",
        "    videos.append(iio.imread(video, index=None))\n",
        "\n",
        "# ndarray with (num_frames, height, width, channel)\n",
        "print(videos[0].shape)  # (36, 150, 200, 3)\n",
        "print(len(videos))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiYk0Ok9TMP5"
      },
      "source": [
        "## From gifs to tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prbtf2GzTMP6"
      },
      "source": [
        "Define function to convert gif to tensor; the function takes the gif path as an argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jsGcLSeRTMP6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image, ImageSequence, ImageOps\n",
        "\n",
        "CHANNELS_TO_MODE = {\n",
        "    1 : 'L',\n",
        "    3 : 'RGB',\n",
        "    4 : 'RGBA'\n",
        "}\n",
        "\n",
        "def seek_all_images(img, channels = 3):\n",
        "    assert channels in CHANNELS_TO_MODE, f'channels {channels} invalid'\n",
        "    mode = CHANNELS_TO_MODE[channels]\n",
        "\n",
        "    i = 0\n",
        "    while True:\n",
        "        try:\n",
        "            img.seek(i)\n",
        "            #yield img.convert(mode)\n",
        "\n",
        "            #yield resized frame; TODO: check it is not always the first frame\n",
        "            yield ImageOps.fit(img, (640,640)).convert(mode)\n",
        "        except EOFError:\n",
        "            break\n",
        "        i += 1\n",
        "\n",
        "def gif_to_tensor(path, channels = 3, transform = T.ToTensor()):\n",
        "    img = Image.open(path) \n",
        "    tensors = tuple(map(transform, seek_all_images(img, channels = channels)))\n",
        "    return torch.stack(tensors, dim = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zc3aMUoaTMP7",
        "outputId": "faca05bb-943f-4e51-d1fb-8f5a0100dca6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50, 3, 10, 640, 640])\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "gifs = []\n",
        "i = 0\n",
        "for gif in Path(\"/content/gdrive/MyDrive/nrr95p/2018_gifs_a\").iterdir():\n",
        "    if not gif.is_file() or i > 49:\n",
        "        continue\n",
        "    gifs.append(gif_to_tensor(gif))\n",
        "    i+=1\n",
        "\n",
        "videos = torch.stack(gifs)\n",
        "print(videos.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVHoi8AATMP8"
      },
      "source": [
        "## Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOaRtAZvTMP8"
      },
      "outputs": [],
      "source": [
        "#import torch\n",
        "from video_diffusion_pytorch import Unet3D, GaussianDiffusion\n",
        "\n",
        "model = Unet3D(\n",
        "    dim = 64,\n",
        "    dim_mults = (1, 2, 4, 8)\n",
        ")\n",
        "\n",
        "diffusion = GaussianDiffusion(\n",
        "    model,\n",
        "    image_size = 640,\n",
        "    num_frames = 10,\n",
        "    timesteps = 1000,   # number of steps\n",
        "    loss_type = 'l1'    # L1 or L2\n",
        ")\n",
        "\n",
        "#videos = torch.randn(1, 3, 5, 32, 32) # video (batch, channels, frames, height, width) - normalized from -1 to +1\n",
        "loss = diffusion(videos)\n",
        "loss.backward()\n",
        "# after a lot of training\n",
        "\n",
        "sampled_videos = diffusion.sample(batch_size = 4)\n",
        "sampled_videos.shape # (4, 3, 5, 32, 32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "8jcyKiYQY8ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from video_diffusion_pytorch import Unet3D, GaussianDiffusion, Trainer\n",
        "\n",
        "model = Unet3D(\n",
        "    dim = 64,\n",
        "    dim_mults = (1, 2, 4, 8),\n",
        ")\n",
        "\n",
        "diffusion = GaussianDiffusion(\n",
        "    model,\n",
        "    image_size = 64,\n",
        "    num_frames = 10,\n",
        "    timesteps = 1000,   # number of steps\n",
        "    loss_type = 'l1'    # L1 or L2\n",
        ").cuda()\n",
        "\n",
        "trainer = Trainer(\n",
        "    diffusion,\n",
        "    '/content/gdrive/MyDrive/nrr95p/2018_gifs_a', # this folder path needs to contain all your training data, as .gif files, of correct image size and number of frames\n",
        "    train_batch_size = 32,\n",
        "    train_lr = 1e-4,\n",
        "    save_and_sample_every = 1000,\n",
        "    train_num_steps = 700000,         # total training steps\n",
        "    gradient_accumulate_every = 2,    # gradient accumulation steps\n",
        "    ema_decay = 0.995,                # exponential moving average decay\n",
        "    amp = True                        # turn on mixed precision\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "l2sZyB2AZA6J"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "1aaf1dd3f9839164dcd234dfa3d5790dd913a3d4e59c4052fb154b7e4ab27a60"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}